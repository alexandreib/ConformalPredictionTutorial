{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh2hGhGcjS528icOxulLk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandreib/ConformalPredictionTutorial/blob/main/JS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # !export KAGGLE_CONFIG_DIR=/content/drive/MyDrive/kaggle.json\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ogQ5L-2qGQrC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle competitions  download -c 'jane-street-real-time-market-data-forecasting' -p '/content/drive/MyDrive/kaggle/'"
      ],
      "metadata": {
        "id": "5MMYD6gUGRer"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/kaggle/jane-street-real-time-market-data-forecasting.zip -d /content/drive/MyDrive/kaggle/"
      ],
      "metadata": {
        "id": "ev8oecpIGTK1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna==4.0.0, catboost==1.2.7, lightgbm==4.2.0, xgboost==2.0.3, tensorflow==2.16.1, scikit-learn==1.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQVc3cChWmFS",
        "outputId": "72869600-01a3-4b2e-fed4-da27f1a1091c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna==4.0.0 in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: catboost==1.2.7 in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: lightgbm==4.2.0 in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: xgboost==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (1.13.3)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (6.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost==1.2.7) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost==1.2.7) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost==1.2.7) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost==1.2.7) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost==1.2.7) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost==1.2.7) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.37.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna==4.0.0) (1.3.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost==1.2.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost==1.2.7) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost==1.2.7) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna==4.0.0) (3.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.2.7) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.2.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.2.7) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.2.7) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.2.7) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.2.7) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost==1.2.7) (9.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc,os, sys, warnings, ctypes, re, joblib, copy, json, collections, abc, tqdm, multiprocessing, random\n",
        "libc = ctypes.CDLL('libc.so.6');\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import (plot_optimization_history,plot_param_importances,plot_parallel_coordinate)\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "import lightgbm, catboost, tensorflow, xgboost\n",
        "print('Optuna Version',optuna.__version__)\n",
        "print('LightGBM Version',lightgbm.__version__)\n",
        "print('CatBoost Version',catboost.__version__)\n",
        "print('XGBoost Version',xgboost.__version__)\n",
        "print('TF Version',tensorflow.__version__)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedGroupKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
        "\n",
        "def is_interactive():\n",
        "    return True #True False\n",
        "\n",
        "def is_kaggle_gpu_enabled():\n",
        "    from tensorflow.python.client import device_lib # when only CPU is enabled the list shows two CPU entries, otherwise there are more, listing GPU as well\n",
        "    return len(device_lib.list_local_devices()) > 2\n",
        "\n",
        "def in_colab():\n",
        "    return True if 'google.colab' in str(get_ipython()) else False\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import output\n",
        "    output.no_vertical_scroll()\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else :\n",
        "    import kaggle_evaluation.jane_street_inference_server\n",
        "    def is_interactive():\n",
        "        return 'runtime' in get_ipython().config.IPKernelApp.connection_file\n",
        "\n",
        "print('in_colab?', in_colab())\n",
        "print('is_kaggle_gpu_enabled?', is_kaggle_gpu_enabled())\n",
        "print('Interactive?', is_interactive())\n",
        "\n",
        "class dotdict(collections.defaultdict):\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "    def __str__(self):\n",
        "        return str(' | '.join([f\"{key}: {value}\" for key, value in self.items()]))\n",
        "    def __repr__(self):\n",
        "        return str(' | '.join([f\"{key}: {value}\" for key, value in self.items()]))"
      ],
      "metadata": {
        "id": "xqDt4xeOGXkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = dotdict(dict)\n",
        "#\n",
        "CFG.l_models = ['catboost', 'xgboost'] #catboost, lgbm, xgboost, nn\n",
        "CFG.fold_name = 'time_serie_split' #time_serie_split , add blocked_time_serie_split\n",
        "CFG.fold_n = 5 if not is_interactive() else 2\n",
        "CFG.fold_train_test_split = 0.8\n",
        "CFG.l_optuna = []\n",
        "CFG.device = 'gpu' if is_kaggle_gpu_enabled() else 'cpu'\n",
        "#\n",
        "CFG.drop =['responder_0','responder_1','responder_2','responder_3','responder_4','responder_5',  'responder_7', 'responder_8',\n",
        "          'date_id','time_id', 'partition_id',\n",
        "           #'weight', 'responder_6',\n",
        "          ]\n",
        "CFG.categoricals = ['symbol_id']\n",
        "#\n",
        "CFG.cols_onehot = []\n",
        "# Models Parameters\n",
        "CFG.lgbm = dotdict(dict)\n",
        "CFG.lgbm.objective = 'rmse'\n",
        "CFG.lgbm.verbose = -1\n",
        "CFG.lgbm.random_seed = 42\n",
        "CFG.lgbm.num_trees = 30_000 if not is_interactive() else 5 #num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, nrounds, num_boost_round, n_estimators, max_iter\n",
        "CFG.lgbm.learning_rate = 0.03\n",
        "CFG.lgbm.max_depth = 12\n",
        "CFG.lgbm.min_data_in_leaf = 25\n",
        "CFG.lgbm.num_leaves = 64\n",
        "CFG.lgbm.subsample = 1\n",
        "CFG.lgbm.reg_lambda = 0.0005\n",
        "\n",
        "CFG.lgbm.device = CFG.device.lower()\n",
        "CFG.lgbm.gpu_use_dp = True if CFG.device.lower() == 'gpu' else False\n",
        "CFG.lgbm.metric  = 'rmse'\n",
        "CFG.lgbm.extra_trees = True\n",
        "CFG.lgbm.colsample_bytree = 0.8\n",
        "CFG.lgbm.subsample_freq = 1\n",
        "CFG.lgbm.early_stopping_rounds = 100\n",
        "#\n",
        "CFG.catboost = dotdict(dict)\n",
        "CFG.catboost.objective = 'RMSE'\n",
        "CFG.catboost.verbose = -1\n",
        "CFG.catboost.random_seed = 42\n",
        "CFG.catboost.num_trees = 30_000 if not is_interactive() else 5 # num_boost_round, n_estimators, num_trees\n",
        "CFG.catboost.learning_rate = 0.03\n",
        "CFG.catboost.max_depth = 12\n",
        "CFG.catboost.min_data_in_leaf = 25\n",
        "# CFG.catboost.num_leaves = 64\n",
        "# CFG.catboost.subsample = 1 # or add bootstrap_type='Poisson'\n",
        "CFG.catboost.reg_lambda = 0.0005\n",
        "\n",
        "CFG.catboost.task_type = CFG.device.upper()\n",
        "CFG.catboost.eval_metric = 'RMSE'\n",
        "if CFG.device.lower() == 'cpu' : CFG.catboost.colsample_bylevel = 0.8\n",
        "CFG.catboost.random_strength = 0.01\n",
        "# CFG.catboost.grow_policy = 'Lossguide'#  default SymmetricTree' ## Needed for Num_leaves parameter\n",
        "CFG.catboost.early_stopping_rounds = 100\n",
        "#\n",
        "CFG.xgboost = dotdict(dict)\n",
        "CFG.xgboost.objective = 'reg:squarederror'\n",
        "CFG.xgboost.verbosity = 1\n",
        "CFG.xgboost.seed = 42\n",
        "CFG.xgboost.n_estimators = 30_000 if not is_interactive() else 5 # num_boost_round, n_estimators, num_trees\n",
        "CFG.xgboost.learning_rate = 0.03\n",
        "CFG.xgboost.max_depth = 12\n",
        "CFG.xgboost.subsample = 1\n",
        "CFG.xgboost.colsample_bylevel = 0.8\n",
        "CFG.xgboost.reg_lambda = 0.0005\n",
        "CFG.xgboost.device = CFG.device.lower()\n",
        "CFG.xgboost.early_stopping_rounds = 100\n",
        "CFG.xgboost.enable_categorical = True if len(CFG.categoricals) > 0 else False\n",
        "#\n",
        "CFG.nn = dotdict(dict)\n",
        "CFG.nn.epochs = 150 if not is_interactive() else 1\n",
        "CFG.nn.lr = 0.01\n",
        "CFG.nn.lr_start = 1e-5\n",
        "CFG.nn.lr_max = 1e-2\n",
        "CFG.nn.lr_rampup = 2\n",
        "CFG.nn.lr_sustain = 1\n",
        "CFG.nn.lr_decay = 0.7\n",
        "# Flow Parameters\n",
        "CFG.load = False\n",
        "CFG.load_path = ''\n",
        "CFG.l_permutation_importance = []\n",
        "CFG.post_processing = False\n",
        "CFG.data_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/'\n",
        "CFG.save_path = '/kaggle/working/'\n",
        "if in_colab(): CFG.data_path = '/content/drive/MyDrive/kaggle/'\n",
        "if in_colab(): CFG.save_path = '/content/drive/MyDrive/Colab Save/'\n",
        "CFG.n_rows = 47_127_338 if not is_interactive() else 100_000\n",
        "CFG.col_target = 'responder_6'\n",
        "CFG.col_weight = 'weight'# None or column name : 'weight'\n",
        "CFG.weights = [1/len(CFG.l_models) for x in CFG.l_models]\n",
        "#\n",
        "# CFG = json.load(open(CFG.load_path + '/CFG.json', 'r'))\n",
        "json.dump(CFG, open(CFG.save_path + 'CFG.json', 'w'))\n",
        "\n",
        "if not is_interactive():\n",
        "    for key, value in CFG.items() :\n",
        "        print(f'{key} : {value}')"
      ],
      "metadata": {
        "id": "8dbXAGvUGYlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pl.read_parquet(CFG.data_path + 'train.parquet', n_rows = CFG.n_rows)\n",
        "print(train.head(1))\n",
        "print(train.shape)\n",
        "\n",
        "lags = pl.read_parquet(CFG.data_path + 'lags.parquet')\n",
        "print(lags.head(1))\n",
        "print(lags.shape)\n",
        "\n",
        "lag_all  = train['date_id','time_id','symbol_id','responder_0','responder_1','responder_2','responder_3','responder_4','responder_5','responder_6','responder_7','responder_8',]\n",
        "lag_all = lag_all.with_columns(pl.col('date_id') + 1)\n",
        "lag_all = lag_all.rename({'responder_0' : 'responder_0_lag_1',\n",
        "                 'responder_1' : 'responder_1_lag_1',\n",
        "                 'responder_2' : 'responder_2_lag_1',\n",
        "                 'responder_3' : 'responder_3_lag_1',\n",
        "                 'responder_4' : 'responder_4_lag_1',\n",
        "                 'responder_5' : 'responder_5_lag_1',\n",
        "                 'responder_6' : 'responder_6_lag_1',\n",
        "                 'responder_7' : 'responder_7_lag_1',\n",
        "                 'responder_8' : 'responder_8_lag_1'})\n",
        "print(lag_all.head(1))\n",
        "print(lag_all.shape)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "gZ94tnGZGyPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def reduce_memory_usage_pl(df):\n",
        "#     print(f\"Memory usage of dataframe is {round(df.estimated_size('mb'), 2)} MB\")\n",
        "#     Numeric_Int_types = [pl.Int8,pl.Int16,pl.Int32,pl.Int64]\n",
        "#     Numeric_Float_types = [pl.Float32,pl.Float64]\n",
        "#     for col in df.columns:\n",
        "#         col_type = df[col].dtype\n",
        "#         c_min = df[col].min()\n",
        "#         c_max = df[col].max()\n",
        "#         if col_type in Numeric_Int_types:\n",
        "#             if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "#                 df = df.with_columns(df[col].cast(pl.Int8))\n",
        "#             elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "#                 df = df.with_columns(df[col].cast(pl.Int16))\n",
        "#             elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "#                 df = df.with_columns(df[col].cast(pl.Int32))\n",
        "#             elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "#                 df = df.with_columns(df[col].cast(pl.Int64))\n",
        "#         elif col_type in Numeric_Float_types:\n",
        "#             if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "#                 df = df.with_columns(df[col].cast(pl.Float32))\n",
        "#             else :\n",
        "#                 pass\n",
        "#         elif col_type == pl.Utf8:\n",
        "#             df = df.with_columns(df[col].cast(pl.Categorical))\n",
        "#         else:\n",
        "#             pass\n",
        "#     print(f\"Memory usage of dataframe became {round(df.estimated_size('mb'), 2)} MB\")\n",
        "#     return df\n",
        "\n",
        "# train = reduce_memory_usage_pl(train)"
      ],
      "metadata": {
        "id": "Wo-3dGRUHORf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit, Features, Encode ..."
      ],
      "metadata": {
        "id": "ipAsBY3LHPxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FE:\n",
        "    def __init__(self):\n",
        "        self.dic_tfid_vectorizer = {}\n",
        "        self.dic_tfid_selected_feature = {}\n",
        "\n",
        "    def get_new_columns(self, df, lags ) :\n",
        "        global lag_all\n",
        "        if lags is not None:\n",
        "          lag_all = pl.concat([lags , lag_all])\n",
        "          lags_all = lag_all.unique(['date_id','time_id','symbol_id'])\n",
        "          df.join(lags_all, on = ['date_id', 'time_id', 'symbol_id'], how = 'left')\n",
        "        return df\n",
        "\n",
        "    def encoders_fit(self, df) :\n",
        "        ### ONE HOT ENCODER FIT\n",
        "        for col in CFG.cols_onehot :\n",
        "            if col not in cols: cols[col] = dotdict(dict)\n",
        "            cols[col].one_hot_encoder = list(df[col].unique(maintain_order = True))\n",
        "\n",
        "    def encoders_transform(self, df):\n",
        "        ### ONE HOT ENCODER TRANSFORM\n",
        "        for i, col in enumerate(CFG.cols_onehot) :\n",
        "            for unique in cols[col].one_hot_encoder:\n",
        "                df = df.with_columns((df[col] == unique).cast(pl.Int8).alias(f'ofe_{col}_{i}'))\n",
        "        return df\n",
        "\n",
        "    def feature_engineering(self, df) :\n",
        "        return df\n",
        "\n",
        "    def scaler_fit(self, df):\n",
        "        cols.numercials_means = df[cols.numericals].mean().to_dicts()[0]\n",
        "        cols.numercials_std = df[cols.numericals].std().to_dicts()[0]\n",
        "\n",
        "    def scaler_transform(self, df):\n",
        "        for col in cols.numericals:\n",
        "            df = df.with_columns( (pl.col(col) - cols.numercials_means[col]) / cols.numercials_std[col])\n",
        "        return df\n",
        "\n",
        "    def clean(self, df):\n",
        "        ## DROP\n",
        "        df = df.drop([col for col in cols.drop if col in df.columns])\n",
        "\n",
        "        ## FILL / CAST\n",
        "        if len( CFG.categoricals ) > 0 : df[cols.categoricals] = df[cols.categoricals].fill_null('nan')\n",
        "        df = df.with_columns([pl.col(col).cast(pl.String).cast(pl.Categorical) for col in CFG.categoricals if col in df.columns]) #\n",
        "        for col in cols.numericals:\n",
        "            if col in df.columns:\n",
        "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
        "\n",
        "        ##PANDA\n",
        "        df = df.to_pandas()\n",
        "        return df"
      ],
      "metadata": {
        "id": "iFi8c0TaHRqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = dotdict(dict)\n",
        "\n",
        "## Data Pipeline\n",
        "print(f'Shape train: {train.shape}')\n",
        "fe = FE()\n",
        "train = fe.get_new_columns(train, lags)\n",
        "\n",
        "fe.encoders_fit(train)\n",
        "print(f'Shape train: {train.shape}')"
      ],
      "metadata": {
        "id": "d9byK1T5HS83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Folds Creation"
      ],
      "metadata": {
        "id": "qce65rjwHU1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fold():\n",
        "    ## Kfold done before concat with df_duplicated\n",
        "    def prepare(self, df) :\n",
        "        date_id = df['date_id'].to_pandas()\n",
        "        uniques = list(date_id.unique())\n",
        "        df_folds = pd.DataFrame(date_id)\n",
        "\n",
        "        plt.figure(figsize=(20,5))\n",
        "        plt.legend('',frameon=False)\n",
        "        self.folds = {}\n",
        "        if CFG.fold_name == 'time_serie_split' :\n",
        "          for fold in range(CFG.fold_n) :\n",
        "              split = int(len(uniques) * CFG.fold_train_test_split)\n",
        "              train_dates = uniques[:split]\n",
        "              valid_dates = uniques[split:]\n",
        "              train_index = list(date_id[date_id.isin(train_dates)].index)\n",
        "              valid_index = list(date_id[date_id.isin(valid_dates)].index)\n",
        "              self.folds[fold] = (train_index, valid_index)\n",
        "              df_folds.loc[train_index, fold] = 'train'\n",
        "              df_folds.loc[valid_index, fold] = 'valid'\n",
        "              uniques = train_dates\n",
        "              df_folds['fold'] = fold\n",
        "              ax = sns.lineplot(df_folds[['date_id','fold', fold]].dropna(subset=fold), x='date_id', y = 'fold', hue = fold, linewidth = 30)\n",
        "              ax.get_legend().remove()\n",
        "          plt.show()\n",
        "\n",
        "        if CFG.fold_name == 'blocked_time_serie_split' :\n",
        "          split = int(len(uniques) / CFG.fold_n) + 1\n",
        "          for fold in range(CFG.fold_n) :\n",
        "            fold_l_date_id = uniques[split * fold : split * (fold + 1)]\n",
        "            fold_index= list(date_id[date_id.isin(fold_l_date_id)].index)\n",
        "\n",
        "            train_index = fold_index[:int(len(fold_index) * CFG.fold_train_test_split)]\n",
        "            valid_index = fold_index[int(len(fold_index) * CFG.fold_train_test_split):]\n",
        "            self.folds[fold] = (train_index, valid_index)\n",
        "            df_folds.loc[train_index, fold] = 'train'\n",
        "            df_folds.loc[valid_index, fold] = 'valid'\n",
        "            df_folds['fold'] = fold\n",
        "            ax = sns.lineplot(df_folds[['date_id','fold', fold]].dropna(subset=fold), x='date_id', y = 'fold', hue = fold, linewidth = 30)\n",
        "            ax.get_legend().remove()\n",
        "          plt.show()\n",
        "        return df\n",
        "\n",
        "    def get_index(self, fold) :\n",
        "        return self.folds[fold]\n",
        "\n",
        "fld = fold()\n",
        "print(f'Shape train: {train.shape}')\n",
        "train = fld.prepare(train)\n",
        "print(f'Shape train: {train.shape}')"
      ],
      "metadata": {
        "id": "pVGhMDNeHUFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform and Feature Engineering"
      ],
      "metadata": {
        "id": "EaLo7Ab9HYl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Shape train: {train.shape}')\n",
        "train = fe.encoders_transform(train)\n",
        "train = fe.feature_engineering(train)\n",
        "print(f'Shape train: {train.shape}')\n",
        "train[CFG.categoricals].head(1)\n",
        "train.head(1)"
      ],
      "metadata": {
        "id": "0VSNgog5HZVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Columns Selection"
      ],
      "metadata": {
        "id": "Z22Hou51IK5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols.drop = copy.deepcopy(CFG.drop)\n",
        "# cols_onehot\n",
        "cols.drop += CFG.cols_onehot\n",
        "# cols_percentages_to_drop\n",
        "# cols_percentages_to_drop = [col for col in train.columns if train[col].value_counts(normalize = True).max()['proportion'].item() > 0.99]\n",
        "# print('cols_percentages_to_drop : ', cols_percentages_to_drop)\n",
        "# cols.drop += cols_percentages_to_drop\n",
        "cols.drop = list(dict.fromkeys(cols.drop)) #removing duplicates\n",
        "\n",
        "cols.categoricals = list(dict.fromkeys(copy.deepcopy(CFG.categoricals)))\n",
        "### Priotity to CAT cols => if col in cat it will not be dropped\n",
        "cols.drop = [col for col in cols.drop if col not in cols.categoricals]\n",
        "### Drop categoricals col which are one hot encoded\n",
        "cols.categoricals = [col for col in cols.categoricals if col not in CFG.cols_onehot]\n",
        "\n",
        "cols.numericals = [col for col in train.columns if (col not in cols.categoricals + cols.drop + [CFG.col_target, CFG.col_weight] and train[col].dtype.is_numeric())]\n",
        "cols.numericals = list(dict.fromkeys(cols.numericals))\n",
        "# double_check_cols = [col for col in train.columns if col not in cols.categoricals + cols.drop + cols.numericals + [CFG.col_target]]\n",
        "# cols.drop +=  double_check_cols\n",
        "# print(f\"double_check_cols : {double_check_cols}\")\n",
        "\n",
        "cols.drop = list(dict.fromkeys(cols.drop)) #removing duplicates\n",
        "print(f\"len(drop) : {len(cols.drop)}\")\n",
        "print(cols.drop)"
      ],
      "metadata": {
        "id": "KJzRFZtdH2UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols.lgbm = dotdict(dict)\n",
        "cols.catboost = dotdict(dict)\n",
        "cols.xgboost = dotdict(dict)\n",
        "cols.nn = dotdict(dict)\n",
        "\n",
        "cols.lgbm.drop = copy.deepcopy(cols.drop)\n",
        "cols.catboost.drop = copy.deepcopy(cols.drop)\n",
        "cols.xgboost.drop = copy.deepcopy(cols.drop)\n",
        "cols.nn.drop = copy.deepcopy(cols.drop)\n",
        "\n",
        "############### ADD SPECIFIC drops per models\n",
        "cols.lgbm.drop = list(dict.fromkeys(cols.lgbm.drop))\n",
        "cols.catboost.drop = list(dict.fromkeys(cols.catboost.drop))\n",
        "cols.xgboost.drop = list(dict.fromkeys(cols.xgboost.drop))\n",
        "cols.nn.drop = list(dict.fromkeys(cols.nn.drop))\n",
        "###############\n",
        "\n",
        "cols.lgbm.features = list(dict.fromkeys([col for col in cols.numericals + cols.categoricals if col not in cols.lgbm.drop]))\n",
        "cols.catboost.features  = list(dict.fromkeys([col for col in cols.numericals + cols.categoricals if col not in cols.catboost.drop]))\n",
        "cols.xgboost.features = list(dict.fromkeys([col for col in cols.numericals + cols.categoricals if col not in cols.xgboost.drop]))\n",
        "cols.nn.features = list(dict.fromkeys([col for col in cols.numericals + cols.categoricals if col not in cols.nn.drop]))\n",
        "\n",
        "cols.lgbm.categoricals = list(dict.fromkeys([col for col in cols.categoricals if col not in cols.lgbm.drop]))\n",
        "cols.catboost.categoricals = list(dict.fromkeys([col for col in cols.categoricals if col not in cols.catboost.drop]))\n",
        "cols.xgboost.categoricals = list(dict.fromkeys([col for col in cols.categoricals if col not in cols.xgboost.drop]))\n",
        "cols.nn.categoricals = list(dict.fromkeys([col for col in cols.categoricals if col not in cols.nn.drop]))\n",
        "\n",
        "cols.lgbm.numericals = list(dict.fromkeys([col for col in cols.numericals if col not in cols.lgbm.drop]))\n",
        "cols.catboost.numericals = list(dict.fromkeys([col for col in cols.numericals if col not in cols.catboost.drop]))\n",
        "cols.xgboost.numericals = list(dict.fromkeys([col for col in cols.numericals if col not in cols.xgboost.drop]))\n",
        "cols.nn.numericals  = list(dict.fromkeys([col for col in cols.numericals if col not in cols.nn.drop]))\n",
        "\n",
        "json.dump(cols, open(CFG.save_path + 'cols.json', 'w'))\n",
        "print('*' * 70)\n",
        "print(f\"number categoricals cols : {len(cols.categoricals)}, number numericals cols : {len(cols.numericals)}, number drop cols : {len(cols.drop)}, number target : 1\")\n",
        "print(f\"total = {len(cols.categoricals + cols.numericals + cols.drop + [CFG.col_target] + [CFG.col_weight])} == {train.shape[1]} : train.shape\")\n",
        "print(f\"cat categoricals : {cols.categoricals}\")\n",
        "print(f\"debug : {[col for col in cols.categoricals + cols.numericals + cols.drop if col not in train.columns]}\")\n",
        "\n",
        "print('*' * 70)\n",
        "print(f\"len(cols.lgbm.drop) :{len(cols.lgbm.drop)}\")\n",
        "print(f\"len(cols.catboost.drop) :{len(cols.catboost.drop)}\")\n",
        "print(f\"len(cols.xgboost.drop) :{len(cols.xgboost.drop)}\")\n",
        "print(f\"len(cols.nn.drop) :{len(cols.nn.drop)}\")\n",
        "\n",
        "print('*' * 70)\n",
        "print(f\"lgbm.numericals: {len(cols.lgbm.numericals)}, lgbm.categoricals : {len(cols.lgbm.categoricals)}\")\n",
        "print(f\"catboost.numericals : {len(cols.catboost.numericals)}, catboost.categoricals : {len(cols.catboost.categoricals)}\")\n",
        "print(f\"xgboost.numericals : {len(cols.xgboost.numericals)}, xgboost.categoricals : {len(cols.xgboost.categoricals)}\")\n",
        "print(f\"nn.numericals: {len(cols.nn.numericals)}, nn.categoricals : {len(cols.nn.categoricals)}\")\n",
        "\n",
        "print('*' * 70)\n",
        "for name in CFG.l_models :\n",
        "    #if condition returns True, then nothing happens:\n",
        "    assert CFG.col_target not in cols[name].numericals, f'CFG.col_target is in cols.{name}.numericals'\n",
        "    assert CFG.col_target not in cols[name].categoricals, f'CFG.col_target is in cols.{name}.numericals'\n",
        "    assert CFG.col_weight not in cols[name].numericals, f'CFG.col_weight is in cols.{name}.numericals'\n",
        "    assert CFG.col_weight not in cols[name].categoricals, f'CFG.col_weight is in cols.{name}.numericals'"
      ],
      "metadata": {
        "id": "rCsCkzJzIPJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale"
      ],
      "metadata": {
        "id": "DNc4lLqnIQlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fe.scaler_fit(train)\n",
        "train = fe.scaler_transform(train)\n",
        "\n",
        "print(f'Shape: {train.shape}')\n",
        "train.head(1)"
      ],
      "metadata": {
        "id": "OOa0uih_IUE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean"
      ],
      "metadata": {
        "id": "laRlZRKxIVK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = fe.clean(train)\n",
        "\n",
        "print(f'Shape: {train.shape}')\n",
        "print('Memory usage: {:.2f} MB\\n'.format(train.memory_usage(index=True).sum() / 1024**2))\n",
        "display(train.head(1))\n",
        "display(train[CFG.categoricals].head(1))"
      ],
      "metadata": {
        "id": "7U83jpK1IYHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_cols_drop = [col for col in train.columns if train[col].isnull().sum() == train.shape[0]]\n",
        "# print(new_cols_drop)\n",
        "# new_cols_drop = [col for col in train.columns if train[col].isna().sum() == train.shape[0]]\n",
        "# print(new_cols_drop)\n",
        "# new_cols_drop = [col for col in train.columns if train[col].nunique() == 1]\n",
        "# print(new_cols_drop)\n",
        "\n",
        "# duplicateColumnNames = set()\n",
        "# for x in range(train.shape[1]):\n",
        "#     for y in range(x + 1, train.shape[1]):\n",
        "#         if train.iloc[:, x].equals(train.iloc[:, y]):\n",
        "#             duplicateColumnNames.add(train.columns[y])\n",
        "# print(list(duplicateColumnNames))"
      ],
      "metadata": {
        "id": "oWWnZAiMIajp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "gIY1YQ6xIcp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lrfn(epoch):\n",
        "    if epoch < CFG.nn.lr_rampup:\n",
        "        lr = (CFG.nn.lr_max - CFG.nn.lr_start) / CFG.nn.lr_rampup * epoch + CFG.nn.lr_start\n",
        "    elif epoch < CFG.nn.lr_rampup + CFG.nn.lr_sustain:\n",
        "        lr = CFG.nn.lr_max\n",
        "    else:\n",
        "        lr = CFG.nn.lr_max * CFG.nn.lr_decay **((epoch - CFG.nn.lr_rampup -  CFG.nn.lr_sustain)//2)\n",
        "    return lr\n",
        "\n",
        "rng = [i for i in range(CFG.nn.epochs)]\n",
        "lr_y = [lrfn(x) for x in rng]\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(rng, lr_y, '-o')\n",
        "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(lr_y[0], max(lr_y), lr_y[-1]))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Learning Rate Schedule\")\n",
        "plt.show()\n",
        "\n",
        "lr_callback = tensorflow.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
        "es_callback = tensorflow.keras.callbacks.EarlyStopping (monitor = 'val_root_mean_squared_error', patience = 10, verbose = 1, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "75oevX2VIeTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom R2 metric for XGBoost\n",
        "def r2_xgboost(y_true, y_pred, sample_weight= None):\n",
        "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
        "    return -r2\n",
        "\n",
        "# Custom R2 metric for LightGBM\n",
        "def r2_lgb(y_true, y_pred, sample_weight):\n",
        "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
        "    return 'r2', r2, True\n",
        "\n",
        "# Custom R2 metric for CatBoost\n",
        "class r2_cbt(object):\n",
        "    def get_final_error(self, error, weight):\n",
        "        return 1 - error / (weight + 1e-38)\n",
        "\n",
        "    def is_max_optimal(self):\n",
        "        return True\n",
        "\n",
        "    def evaluate(self, approxes, target, weight):\n",
        "        assert len(approxes) == 1\n",
        "        assert len(target) == len(approxes[0])\n",
        "        approx = approxes[0]\n",
        "        error_sum = 0.0\n",
        "        weight_sum = 0.0\n",
        "        for i in range(len(approx)):\n",
        "            w = 1.0 if weight is None else weight[i]\n",
        "            weight_sum += w * (target[i] ** 2)\n",
        "            error_sum += w * ((approx[i] - target[i]) ** 2)\n",
        "        return error_sum, weight_sum\n",
        "\n",
        "CFG.lgbm.eval_metric = [r2_lgb]\n",
        "CFG.catboost.eval_metric = r2_cbt()\n",
        "CFG.xgboost.eval_metric = r2_xgboost\n",
        "CFG.xgboost.disable_default_eval_metric = True\n",
        "# CFG.nn.eval_metric ="
      ],
      "metadata": {
        "id": "t-cSvZVBIfie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model(abc.ABC) :\n",
        "    def __init__(self, name, params):\n",
        "        self.name = name\n",
        "        if params is None : self.params = CFG[self.name]\n",
        "        else: self.params = params\n",
        "        print(f'Create model : {self.name}')\n",
        "        print(f'Params: {self.params}')\n",
        "\n",
        "    def load(self, fold) :\n",
        "        self.model = joblib.load(CFG.load_path + f'/{fold}_{self.name}_model')\n",
        "\n",
        "    def save(self, fold) :\n",
        "        joblib.dump(self.model, CFG.save_path + f'{fold}_{self.name}_model')\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def fit(self, fold, X_train, X_valid, y_train, y_valid, w_train = None, w_valid = None) :\n",
        "        pass\n",
        "\n",
        "    def predict(self, df) :\n",
        "        return self.model.predict(df[cols[self.name].features])\n",
        "\n",
        "    def get_feature_importance(self) :\n",
        "        return dict(zip(cols[self.name].features, self.model.feature_importances_))\n",
        "\n",
        "class model_nn(model) :\n",
        "    def __init__(self, name, params):\n",
        "        super().__init__(name, params)\n",
        "        x_input_cats = tensorflow.keras.layers.Input(shape=(len(cols[self.name].categoricals),))\n",
        "        embs = []\n",
        "        for j, col in enumerate(cols.nn.categoricals):\n",
        "            e = tensorflow.keras.layers.Embedding(cols[col].cat_size, cols[col].cat_emb)\n",
        "            x = e(x_input_cats[:,j])\n",
        "            x = tensorflow.keras.layers.Flatten()(x)\n",
        "            embs.append(x)\n",
        "\n",
        "        # NUMERICAL FEATURES\n",
        "        x_input_nums = tensorflow.keras.layers.Input(shape=(len(cols[self.name].numericals),))\n",
        "\n",
        "        # COMBINE\n",
        "        x = tensorflow.keras.layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "        x = tensorflow.keras.layers.Dense(256, activation='relu')(x)\n",
        "        x = tensorflow.keras.layers.Dense(256, activation='relu')(x)\n",
        "        x = tensorflow.keras.layers.Dense(256, activation='relu')(x)\n",
        "        x = tensorflow.keras.layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "        self.model = tensorflow.keras.models.Model(inputs=[x_input_cats, x_input_nums], outputs=x)\n",
        "        self.model.compile(optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
        "                           loss=\"mean_squared_error\",\n",
        "                           metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "    def fit(self, fold, X_train, X_valid, y_train, y_valid, w_train = None, w_valid = None) :\n",
        "        self.model.fit([X_train[cols[self.name].categoricals].astype(int).values, X_train[cols[self.name].numericals].values],\n",
        "                       y_train,\n",
        "                       validation_data = ([X_valid[cols[self.name].categoricals].astype(int).values, X_valid[cols[self.name].numericals].values], y_valid),\n",
        "                       callbacks = [lr_callback, es_callback],\n",
        "                       batch_size=64, epochs=self.params.epochs, verbose=2)\n",
        "\n",
        "    def predict(self, df) :\n",
        "        return self.model.predict([df[cols[self.name].categoricals].astype(int).values, df[cols[self.name].numericals].values], verbose = 0).flatten()\n",
        "\n",
        "    def get_feature_importance(self) :\n",
        "        col_names = []\n",
        "        for col in cols[self.name].categoricals :\n",
        "            for i in range(cols[col].cat_emb) :\n",
        "                col_names.append(f'{col}_{i}')\n",
        "\n",
        "        self = md.models['nn'][0]\n",
        "        for idx, layer in enumerate(self.model.layers) :\n",
        "            if 'concatenate' in layer.name :\n",
        "                break\n",
        "        weights = np.abs(self.model.layers[idx+1].get_weights()[0][:,0])\n",
        "        return dict(zip(col_names + cols[self.name].numericals, weights))\n",
        "\n",
        "class model_lgbm(model) :\n",
        "    def __init__(self, name, params):\n",
        "        super().__init__(name, params)\n",
        "        self.model = lightgbm.LGBMRegressor(**self.params)\n",
        "\n",
        "    def fit(self, fold, X_train, X_valid, y_train, y_valid, w_train = None, w_valid = None) :\n",
        "        self.model.fit(X_train[cols[self.name].features], y_train, w_train,\n",
        "                        eval_set=[(X_valid[cols[self.name].features], y_valid, w_valid)],\n",
        "                        categorical_feature = cols[self.name].categoricals,\n",
        "                        callbacks=[lightgbm.early_stopping(CFG.early_stop, verbose=1),\n",
        "                                  lightgbm.log_evaluation(100)])\n",
        "\n",
        "\n",
        "class model_catboost(model) :\n",
        "    def __init__(self, name, params):\n",
        "        super().__init__(name, params)\n",
        "        self.model = catboost.CatBoostRegressor(**self.params)\n",
        "\n",
        "    def fit(self, fold, X_train, X_valid, y_train, y_valid, w_train = None, w_valid = None) :\n",
        "        # trainset = catboost.Pool(X_valid, y_valid, weight=w_valid, cat_features= cols[self.name].categoricals)\n",
        "        validset = catboost.Pool(X_valid[cols[self.name].features], y_valid, weight=w_valid, cat_features= cols[self.name].categoricals)\n",
        "        self.model.fit(X_train[cols[self.name].features], y_train, sample_weight=w_train,\n",
        "                        eval_set=[validset],\n",
        "                        cat_features = cols[self.name].categoricals,\n",
        "                        early_stopping_rounds = CFG.early_stop,\n",
        "                        verbose = 100)\n",
        "\n",
        "class model_xgboost(model) :\n",
        "    def __init__(self, name, params):\n",
        "        super().__init__(name, params)\n",
        "        self.model = xgboost.XGBRegressor(**self.params)\n",
        "        # self.model = xgboost.Booster(**self.params)\n",
        "\n",
        "    def fit(self, fold, X_train, X_valid, y_train, y_valid, w_train = None, w_valid = None) :\n",
        "        dic_fit={}\n",
        "        if w_train is not None : dic_fit['sample_weight']= [w_train]\n",
        "        if w_valid is not None : dic_fit['sample_weight_eval_set']= [w_valid]\n",
        "        self.model.fit(X_train[cols[self.name].features], y_train,\n",
        "                        eval_set=[(X_valid[cols[self.name].features], y_valid)],\n",
        "                        verbose = 100,\n",
        "                        # categorical_feature = cols[self.name].categoricals,\n",
        "                        **dic_fit)\n",
        "        # self.model = xgboost.train\n",
        "\n",
        "## Factory\n",
        "class Model_Factory() :\n",
        "    def get_model(name, params = None):\n",
        "        if name == \"lgbm\":\n",
        "            return model_lgbm(name, params)\n",
        "        elif name == \"catboost\":\n",
        "            return model_catboost(name, params)\n",
        "        elif name == \"nn\":\n",
        "            return model_nn(name, params)\n",
        "        elif name == \"xgboost\":\n",
        "            return model_xgboost(name, params)\n",
        "        else:\n",
        "            raise TypeError(\"Specify a valid name model\")"
      ],
      "metadata": {
        "id": "OUIpklfzIggI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MD:\n",
        "    def __init__(self):\n",
        "        self.models = collections.defaultdict(list)\n",
        "        self.models_scores = collections.defaultdict(list)\n",
        "        self.oof_preds_scores = []\n",
        "\n",
        "    def get_trained_model(self, name, fold, X_train, X_valid, y_train, y_valid, w_train=None, w_valid=None) :\n",
        "        model = Model_Factory.get_model(name)\n",
        "        if CFG.load :\n",
        "            model.load(fold)\n",
        "        else :\n",
        "            model.fit(fold, X_train, X_valid, y_train, y_valid, w_train, w_valid)\n",
        "        return model\n",
        "\n",
        "    def train(self):\n",
        "        print('*' * 70)\n",
        "        print(f\"{'*' * 30} TRAINING {'*' * 30}\"[:70])\n",
        "        print('*' * 70)\n",
        "        X = train[cols.categoricals + cols.numericals]\n",
        "        y = train[CFG.col_target]\n",
        "        if CFG.col_weight is not None : w = train[CFG.col_weight]\n",
        "\n",
        "        self.oof_preds = np.zeros(len(y))\n",
        "        self.models_preds = np.zeros((len(y), len(CFG.l_models)))\n",
        "        for fold in range(CFG.fold_n):\n",
        "            print(f\"{'*' * 30} FOLD : {fold} {'*' * 30}\"[:70])\n",
        "            train_index, valid_index = fld.get_index(fold)\n",
        "\n",
        "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "            if CFG.col_weight is not None : w_train, w_valid = w.iloc[train_index], w.iloc[valid_index]\n",
        "            else : w_train, w_valid = None, None\n",
        "\n",
        "            for name in CFG.l_models :\n",
        "                print(f\"{'*' * 30} MODEL : {name} {'*' * 30}\"[:70])\n",
        "                model = self.get_trained_model(name, fold, X_train, X_valid, y_train, y_valid, w_train, w_valid)\n",
        "                model.save(fold)\n",
        "                self.models[name].append(model)\n",
        "                self.models_preds[valid_index, CFG.l_models.index(name)] = pred = model.predict(X_valid)\n",
        "                self.models_scores[name].append(np.sqrt(mean_squared_error(y_valid, pred)))\n",
        "\n",
        "            self.oof_preds[valid_index] = pred = (CFG.weights * self.models_preds[valid_index,:]).sum(axis=1)\n",
        "            self.oof_preds_scores.append(np.sqrt(mean_squared_error(y_valid, pred)))\n",
        "\n",
        "\n",
        "        print('*' * 70)\n",
        "        print(f\"{'*' * 30} OOF RESULTS {'*' * 30}\"[:70])\n",
        "        print('*' * 70)\n",
        "        for i, name in enumerate(CFG.l_models) :\n",
        "            print(f\"{name} OOF scores :{np.sqrt(mean_squared_error(y, self.models_preds[:,i])):.5f}\")\n",
        "            print(f\"{name} mean all scores :{np.mean(self.models_scores[name]):.5f}, std all scores :{np.std(self.models_scores[name]):.5f}, Scores : {self.models_scores[name]}.\")\n",
        "            self.print_feature_importances(name)\n",
        "        print(f\"oof_preds mean scores :{np.mean(self.oof_preds_scores):.5f}, std scores :{np.std(self.oof_preds_scores):.5f}, Scores : {self.oof_preds_scores}.\")\n",
        "\n",
        "    def infer(self, df):\n",
        "        self.models_preds = np.zeros((len(df), len(CFG.l_models)))\n",
        "        for i, name in enumerate(CFG.l_models) :\n",
        "            self.models_preds[:, CFG.l_models.index(name)] = np.mean([model.predict(df[cols[name].features]) for model in self.models[name]], axis = 0)\n",
        "        return (CFG.weights * self.models_preds).sum(axis = 1)\n",
        "\n",
        "    def print_feature_importances(self, name) :\n",
        "        print('*' * 70)\n",
        "        print(f\"{'*' * 30}  FEATURES IMPORTANCE {'*' * 30}\"[:70])\n",
        "        print('*' * 70)\n",
        "        for i, model in enumerate(self.models[name]) :\n",
        "            if i == 0 : feature_importances = np.array(list(model.get_feature_importance().values()))\n",
        "            else : feature_importances += list(model.get_feature_importance().values())\n",
        "        feature_importances = pd.Series(feature_importances, index = list(model.get_feature_importance().keys())).sort_values(ascending=True)\n",
        "\n",
        "        print(f\"{name}_feature_importances les moins importantes: \", list(feature_importances[:20].index))\n",
        "        print(f\"{name}_feature_importances les plus importantes: \", list(feature_importances[-20:].index))\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        sns.barplot(y = feature_importances[-20:].index, x = feature_importances[-20:].values, orient=\"h\")\n",
        "        plt.show()\n",
        "\n",
        "md = MD()"
      ],
      "metadata": {
        "id": "_Q_-oDKPIiYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_search_func() :\n",
        "    print('*' * 70)\n",
        "    print(f\"{'*' * 30} WEIGHT SEARCH {'*' * 30}\"[:70])\n",
        "    print('*' * 70)\n",
        "    pred_values = md.models_preds\n",
        "    true_values = train[CFG.col_target].values\n",
        "\n",
        "    lr = LinearRegression(fit_intercept = False, positive = True)\n",
        "    lr.fit(pred_values, true_values)\n",
        "\n",
        "    print(f\"CFG.weights before : {CFG.weights}\")\n",
        "    weights = lr.coef_/lr.coef_.sum()\n",
        "    dic_weight = dict((model,weights[i]) for i, model in enumerate(CFG.l_models))\n",
        "    print(f\"CFG.weights after : {weights}\")\n",
        "\n",
        "    pred_values_weighted = (pred_values * weights).sum(axis=1)\n",
        "    pred_values_mean = (pred_values).mean(axis=1)\n",
        "\n",
        "    rmse_score_weighted = np.sqrt(mean_squared_error(pred_values_weighted , true_values))\n",
        "    rmse_score_mean = np.sqrt(mean_squared_error(pred_values_mean, true_values))\n",
        "\n",
        "    print(f\"RMSE MEAN : {rmse_score_mean}\")\n",
        "    print(f\"RMSE WEIGTHED : {rmse_score_weighted}\")\n",
        "    print(f\"dic_weight : {dic_weight}\")\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "To7lmxEkIjrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = dotdict(dict)\n",
        "opt.n_trials = 20 if not is_interactive() else 2\n",
        "opt.direction = 'minimize'\n",
        "\n",
        "def run_optimization(objective, n_trials = opt.n_trials , n_jobs = 1):\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "    study = optuna.create_study(direction = opt.direction)\n",
        "    study.optimize(objective, n_trials = n_trials, n_jobs = n_jobs, show_progress_bar = False)\n",
        "    return study\n",
        "\n",
        "\n",
        "def optimize(trial):\n",
        "    opt[name] = CFG[name]\n",
        "    opt[name].learning_rate = trial.suggest_float('learning_rate', 1e-2, 2e-1)\n",
        "    opt[name].min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 1, 100) # Alias min_data_per_leaf, min_data, min_child_samples, min_samples_leaf\n",
        "    opt[name].max_depth =  trial.suggest_int('max_depth', 5, 12) # Alias depth\n",
        "    opt[name].num_leaves =  trial.suggest_int('num_leaves', 6, int((2**opt[name].max_depth) * 0.75))\n",
        "    opt[name].subsample =  trial.suggest_float('subsample', 0.05, 1.0) # Alias sub_row, subsample, bagging\n",
        "    opt[name].reg_lambda = trial.suggest_float('reg_lambda', 1e-3, 1.0) # Alias l2_leaf_reg, lambda_l2 , reg_lambda, lambda, l2_regularization\n",
        "\n",
        "    if name == 'lgbm' :\n",
        "        opt[name].colsample_bytree = trial.suggest_float('colsample_bytree', 0.2, 1.0)\n",
        "        opt[name].subsample_freq = trial.suggest_categorical('subsample_freq', [1, 2, 3]) # Alias bagging_freq\n",
        "    if name == 'catboost' :\n",
        "        opt[name].colsample_bylevel = trial.suggest_float('colsample_bylevel', 0.05, 1.0)\n",
        "\n",
        "    model = Model_Factory.get_model(name, opt[name])\n",
        "    model.fit(fold, X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "    pred = model.predict(X_valid)\n",
        "    score = np.sqrt(mean_squared_error(y_valid, pred))\n",
        "\n",
        "    libc.malloc_trim(0)\n",
        "    gc.collect()\n",
        "    return score\n",
        "\n",
        "for name in CFG.l_optuna :\n",
        "    X = train[cols.categoricals + cols.numericals]\n",
        "    y = train[CFG.col_target]\n",
        "\n",
        "    fold = 0\n",
        "    train_index, valid_index = fld.get_index(fold)\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    study = run_optimization(optimize, n_trials=opt.n_trials, n_jobs=1)\n",
        "    best_params = study.best_params\n",
        "\n",
        "    plot_optimization_history(study).show()\n",
        "    plot_param_importances(study).show()\n",
        "    plot_parallel_coordinate(study).show()\n",
        "\n",
        "    print(f'best_params {name} : {best_params}')\n",
        "    print(f'Current Conf : {CFG[name]}')\n",
        "    CFG[name].update(dotdict(**best_params))\n",
        "    print(f'Updated Conf : {CFG[name]}')\n",
        "    json.dump(best_params, open(CFG.save_path + f'{name}_best_params_optuna.json', 'w'))\n",
        "    libc.malloc_trim(0)\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "W5ctpZVMIk4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def permutation_importance () :\n",
        "    print('*' * 70)\n",
        "    print(f\"{'*' * 30} PERMUTATION IMPORTANCE  {'*' * 30}\"[:70])\n",
        "    print('*' * 70)\n",
        "\n",
        "    if is_interactive() : CFG.l_permutation_importance = CFG.l_permutation_importance[:10]\n",
        "    results = dict([(col, 0) for col in CFG.l_permutation_importance])\n",
        "\n",
        "    y = train[CFG.col_target]\n",
        "    X = train[cols.numericals + cols.categoricals]\n",
        "\n",
        "    mse_score = mean_squared_error(md.oof_preds , y)\n",
        "    print(f\"mse_score : {np.sqrt(mse_score)}\")\n",
        "\n",
        "    for fold in range(CFG.fold_n):\n",
        "        print(f\"{'*' * 30} FOLD : {fold} {'*' * 30}\"[:70])\n",
        "        train_index, valid_index = fld.get_index(fold)\n",
        "        tr, va = X.iloc[train_index], X.iloc[valid_index]\n",
        "        y_tr, y_va = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "        for col in CFG.l_permutation_importance:\n",
        "            save_col = va[col].copy()\n",
        "            va[col] = np.random.permutation(va[col])\n",
        "            if col in cols.categoricals : va[col] = va[col].astype(\"category\")\n",
        "            predicts = []\n",
        "            for name in CFG.l_models :\n",
        "                predicts.append(md.models[name][fold].predict(va[cols[name].features]))\n",
        "            results[col] += (mean_squared_error(np.mean(predicts, axis = 0) ,y_va) - mse_score) / CFG.fold_n\n",
        "            va[col] = save_col\n",
        "\n",
        "    df_perm_impt = pd.DataFrame.from_dict(results, orient='index', columns=['perm_importance'])\n",
        "    df_perm_impt = df_perm_impt.sort_values('perm_importance', ascending=True)\n",
        "    df_perm_impt.to_csv('df_perm_impt.csv')\n",
        "    json.dump(results, open(CFG.save_path + 'result_permutation_importance.json', 'w'))\n",
        "\n",
        "    l_fe_pos_perm = list(df_perm_impt[df_perm_impt.perm_importance > 0].index)\n",
        "    print(f'list {len(l_fe_pos_perm)} features with positives permutation importance : \\n{l_fe_pos_perm}')\n",
        "\n",
        "    l_fe_0_perm = list(df_perm_impt[df_perm_impt.perm_importance == 0].index)\n",
        "    print(f'list {len(l_fe_0_perm)} features with 0 permutation importance :\\n{l_fe_0_perm}')\n",
        "\n",
        "    l_fe_neg_perm = list(df_perm_impt[df_perm_impt.perm_importance < 0].index)\n",
        "    print(f'list {len(l_fe_neg_perm)} features with negative permutation importance :\\n{l_fe_neg_perm}')\n",
        "\n",
        "    abv_zero = df_perm_impt[df_perm_impt.perm_importance > 0]\n",
        "    bel_zero = df_perm_impt[df_perm_impt.perm_importance <= 0]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, max(10, int(len(df_perm_impt)/5))))\n",
        "    bars = ax.barh(bel_zero.index, np.round(bel_zero.perm_importance, 5), height = 0.4, color='r')\n",
        "    ax.bar_label(bars)\n",
        "    bars = ax.barh(abv_zero.index, np.round(abv_zero.perm_importance, 5), height = 0.4, color='g')\n",
        "    ax.bar_label(bars)\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.savefig('perm_impt.png')\n",
        "    plt.show()\n",
        "    del abv_zero, bel_zero, y, X, tr, va, y_tr, y_va, df_perm_impt"
      ],
      "metadata": {
        "id": "H8rhms3bImAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CFG.l_permutation_importance = list(dict.fromkeys(CFG.l_permutation_importance))\n",
        "# CFG.l_permutation_importance = list(dict.fromkeys(cols.catboost.features))"
      ],
      "metadata": {
        "id": "A47UI5f-IoWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libc.malloc_trim(0);\n",
        "gc.collect();\n",
        "\n",
        "md.train()\n",
        "CFG.weights = weight_search_func()\n",
        "if len(CFG.l_permutation_importance) > 0:\n",
        "    permutation_importance()"
      ],
      "metadata": {
        "id": "PvplbcAjInxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}